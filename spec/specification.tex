\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{color}
\usepackage{listings}
\usepackage[linktocpage,colorlinks=true]{hyperref}
\usepackage{graphicx}
\usepackage{multicol}

\begin{document}

\title{PerfOCaml: a Benchmarking Environment for OCaml\\Specification\\Pierre's last edit on: 2014/06/05}

\maketitle
\tableofcontents
\section{Use Cases}

Here are some examples of how users should be able to use PerfOCaml.

\subsection{Micro-benchmarking in the Compiler Directory}

Allows to quickly evaluate the difference between previous versions or
builds of the compiler and the just built (not installed) one.

In the directory where OCaml was built:
\begin{itemize}
\item {\tt operf-micro init {\em name}}: prepare the environment
\item {\tt operf-micro build}: prepare the benchmarking binaries
\item {\tt operf-micro clean}: reinitialize after a rebuild of the compiler
\item {\tt operf-micro run}: generate the benchmark result files
\item {\tt operf-micro build-run}: run both the build and run commands
\item {\tt operf-micro compare {\em path}}: compare the results with the runs saved in the path
\item {\tt operf-micro list}: print the list of benchmark groups
\end{itemize}

Constraints: should not depend on anything external to the just-built
compiler, should run on windows and should be able to compare
completely unrelated versions of the compiler.

\subsection{Macro-benchmarking in the Compiler Directory}

Provides a more complete idea of the performance of a functional
ocaml source tree.

After ensuring that the compiler seems to correctly work and install,
run in the source directory:

\begin{itemize}
\item {\tt operf-ocaml init {\em name}}: prepare the environment (create an opam root, switch, ...)
\item {\tt operf-ocaml configure  --(additional configure options)}: configure ocaml build
\item {\tt operf-ocaml build [list of benchmark suites] }: build ocaml, the
  libraries and programs needed to run the selected benchmarks
\item {\tt operf-ocaml run}: run the selected benchmarks and generate the result files
\item {\tt operf-ocaml compare {\em path}}: compare the results with the runs saved in the path
\item {\tt operf-ocaml clean}: reset to the state after init (can change the compiler source code and launch build with the updated sources).
\item {\tt operf-ocaml list}: list available benchmarks
\end{itemize}

\subsection{Macro-benchmarking an OPAM Switch}

Here, the user wants to benchmark a compiler or a package that is
installed in some existing OPAM switch.

\subsection{Weather service}

The Weather service should be a regularly (automatically) updated web
page comparing different versions of the compilers.

Every day or commit in a compiler branch the compiler should be
rebuilt and run on various sets of benchmarks based on a set of fixed
versions of opam packages (the same for every version of the compiler)

The site should provide graphs of evolution of different aggregated
performance indexes, precise results for each benchmarks, a list of
significant regressions and analyzable raw data (like perf records).

Users using exotic architectures or systems should be able to submit
result associated with precise information about the run environment.

\section{Sub-Projects}

\subsection{Micro-benchmarking in the Compiler Directory}

\subsubsection{The {\tt operf-micro} tool}

In the directory {\tt\em \$OCAMLSRC} where OCaml was built:
\begin{itemize}
\item {\tt operf-micro init {\em name}}\\ Prepare the environment
  for micro-benchmarking the compiler in the same directory.
  Do the following operations:
  \begin{itemize}
  \item Create a sub-directory {\tt operf}, with a config file {\tt config}
  indicating that the name of the current compiler is {\tt\em name}.
  {\tt operf-micro} will complain if the directory
  {\tt \$HOME/.operf/micro/{\em name}/} \footnote{TODO the path should
    follow XDG recommendations} already exists (as it will try to
  save the micro-benchmarks results in that location).
  \item
    if {\tt \$PREFIX/share/operf/micro} exists, copy it to the
    {\tt operf} sub-directory.\\
    otherwise
    \begin{itemize}
    \item Download a compressed archive of the micro-benchmark from a
      default URL. A {\tt [--benchmarks {\em url} ]} switch could be
      used to override the default URL
      (\url{https://github.com/OCamlPro/operf-micro/archive/1.00.0.zip}),
      in particular to specify a local archive.\footnote{It should
        probably use the one provided at installation}
    \item Extract the archive in the {\tt operf} sub-directory
    \end{itemize}
  \end{itemize}
\item {\tt operf-micro build [--byte | --opt] [--args ``...'' | --opt-args ``...'' | --byte-args ``...''] [--name ``...'']}\\
  Create or replace the files
  \begin{itemize}
  \item {\tt operf/micro/timestamp} containing the current time, in the
    {\tt\em YYYY-MM-DD\_hh-mm-ss} format identifying the build.
  \item {\tt operf/name} containing the {\tt --name} argument, to
    annotate the build
  \item {\tt operf/args} {\tt operf/byte-args} {\tt operf/opt-args}
    containing the {\tt --args} argument, to pass compilation options
    to ocaml when building benchmarks. Typically for optimization
    (like --inline or --unsafe)
  \end{itemize}
  Try to build the benchmarks, using the local compiler. The benchmarks will
  assume that the bytecode compiler is:\\
  {\tt \$OCAMLSRC/byterun/ocamlrun \$OCAMLSRC/ocamlc -I \$OCAMLSRC/stdlib}\\
  and the native code compiler is:\\
  {\tt \$OCAMLSRC/byterun/ocamlrun \$OCAMLSRC/ocamlopt -I \$OCAMLSRC/stdlib}\\
  It might also assume that the {\tt bigarray} library is available.
  This step is useful if some changes have to be done to the benchmarks in
  order for them to correctly compile (note that changing the benchmarks
  can lead to false comparisons).
\item {\tt operf-micro clean}\\
  Reinitialize after a rebuild of the compiler
\item {\tt operf-micro run [--fast | --slow | --all] [--name ``...'']
  [--arg n] [-o ``path'']}\\
  Run the micro-benchmarks. If {\tt \$OCAMLSRC/byterun/ocamlrun},
  {\tt \$OCAMLSRC/ocamlc} or {\tt \$OCAMLSRC/ocamlopt} have changed since the
  last compilation, it will first clean the benchmarks. If needed, it will
  compile the benchmarks.

  If {\tt -o path} is specified, record results in this directory,
  otherwise record it in {\tt
    \$HOME/.operf/results/{\em name}/{\em date}/}\footnote{TODO: XDG}
  (the date is the one recorded in {\tt operf/micro/timestamp}.

  If {\tt --all} is specified, all benchmarks are run, if {\tt --slow}
  is specified, slower benchmarks are not run, if none or
  {\tt --fast} is specified, only benchmarks marked as fast
  running are run. \\
  If {\tt --name ``group''} is specified, the functions from this
  group are run. If {\tt --name ``group.function''} is specified, this
  function from this group is run. This option can be added multiple
  times to run multiple functions. If no --name option is present, all
  functions are run. \\
  If {\tt --arg n} is specified with n an integer or a range ``n-m'' of
  integers, parametric benchmarks are run with those arguments. Can
  be specified multiple times

\item {\tt operf-micro build-run [build and run options]}\\
  Execute the build then the run command.
\item {\tt operf-micro compare [({\em name} | {\em date} | {\em
      name:date})]}\\ Compare the latest results with a previous run.
  If {\tt\em name} is provided, it is for another compiler. If {\tt\em
    date} is provided, it is the last run just before or equal to that
  date.
\item {\tt operf-micro save-diff {\em diff-name}}\\
  If modifications where done to the micro-benchmarks, save them in
  {\tt \$HOME/.operf/micro-diffs/{\em diff-name}.patch} for later use.
  An example of meaningful modification would be to replace an old
  construct with a new one only available after a given compiler version.
\item  {\tt operf-micro load-diff {\em diff-name}}\\
  Try to find an existing patch and apply it to the current benchmarks.
\item {\tt operf-micro version {\em ocaml-version ...}}\\ Download OCaml
  sources for version {\tt\em ocaml-version}, build them, create a
  micro-benchmark with name {\tt\em ocaml-version}, and run the
  micro-benchmarks. This can be used to populate the micro-benchmarks
  for comparisons.
\item {\tt operf-micro list}: print the list of benchmark groups
\end{itemize}

The {\tt operf-micro} tool must be able to run on any platform and
simple to build: it should avoid external dependencies, and should be
shippable as a single binary. It should work without
(tar/gunzip/curl/...) but specific commands using them could fail.

\subsubsection{Format of the Micro Benchmarks}

Micro benchmarks can be provided:\label{micro-dir-format}
\begin{itemize}
\item by the operf archive and be installed when installing operf
  binaries. In that case it is present at {\tt
    \$PREFIX/share/operf/micro/}\footnote{operf configuration time
    prefix}
\item by the user: in the {\tt \$HOME/.operf/micro/}\footnote{XDG}
  directory.
\item in an archive file
\end{itemize}

In each case, the directory structure is:

\begin{itemize}
\item {\tt description.txt}: a textual description of what the benchmark does and evaluates
\item Directories whose names are benchmarks names. A directory is
  considered to describe a benchmark if it contains a {\tt
    benchmark.ml} file.
  \begin{itemize}
  \item {\tt benchmark.ml}: containing a {\tt functions} variable of
    type ``Micro\_bench\_types.benchmark list''. It is the list of
    available functions to run.
  \item optional additional .ml files
  \item optional {\tt benchmark.build} file: describe how to compile the
    suite If none provided, a reasonable default is assumed
  \end{itemize}
\end{itemize}

{\tt benchmark.ml}
\begin{verbatim}

let f1 () = 1 + 2
let check_f1 v = v = 3

let f2 n = Array.create n 1
let gen_f2 n = n
let check_f2 n v = Array.length v = n

open Micro_bench_types

let functions =
  [ "function 1",          Unit (f1, check_f1, Fast);
    "some other function", Int (f2, gen_f2, check_f2,
                               [ Range (0, 100000), Fast;
                                 Range (100001, 100000000), Slow;
                                 Range (100000001, 10000000000), Slower ]) ]

\end{verbatim}

{\tt benchmark.build}
\begin{verbatim}
{ files: ["file1.mli" "file1.ml" "file2.ml" "benchmark.ml"],
  link: ["bigarray"],
  opt: ["-rectypes"]
  available_if: ... }
\end{verbatim}
\footnote{available\_if still to define. Does it constraint ocaml version, or features ?}

If available\_if if not verified or if libraries specified in link are
not available, the benchmark won't be built or run.

the default (if not specified) {\tt benchmark.build} is
\begin{verbatim}
{ files: ["benchmark.ml"] }
\end{verbatim}

The files will be compiled with the command line:
\begin{verbatim}
ocamlrun ocamlopt -I stdlib -I bigarray bigarray.cmxa -rectypes $ARGS $OPT_ARGS micro_bench_type.mli file1.mli file1.ml file2.ml main.ml micro_bench_runner.ml -o native
\end{verbatim}
or
\begin{verbatim}
ocamlrun ocamlopt -I stdlib -I bigarray bigarray.cma -rectypes $ARGS $BYTE_ARGS micro_bench_type.mli file1.mli file1.ml file2.ml main.ml micro_bench_runner.ml -o byte
\end{verbatim}


\label{micro_bench_type.mli}{\tt micro\_bench\_type.mli} is fixed and is:
\begin{verbatim}
type result =
  | Ok
  | Error of string

type cost =
  | Fast
  | Slow
  | Slower

type range =
  | Any
  | Range of int * int
  | List of int list

type function_kind =
  | Unit :  ( unit -> 'result )   (* benchmarked function *)
          * ( 'result -> result ) (* test function *)
          * cost                  (* estimated run time *)
          -> function_kind
  | Int : ( 'arg -> 'result )     (* benchmarked function *)
          * ( int -> 'arg )       (* argument generation *)
          * ( 'result -> result ) (* test function *)
          * (range * cost) list   (* estimated run time for parameter range *)
          -> function_kind

type benchmark = string (* function name *)
               * function_kind
\end{verbatim}


\subsubsection{Format of the {\tt \$HOME/.operf/} directories}
\footnote{XDG}
\begin{itemize}
\item {\tt config} file describing general configuration.
  \footnote{None currently, but some needs will certainly emerge}
  General format:
\begin{verbatim}
{ key : "value",
  key2 : "value" }
\end{verbatim}
\item {\tt micro}
  \begin{itemize}
  \item benchmarks structure described in the \ref{micro-dir-format} section
  \item {\tt patch} directory containing .patch files applicable to existing suite
  \end{itemize}
\item {\tt results}: contains both micro and macro benchmark results.
  \begin{itemize}
  \item ``compiler name''/{\tt\em YYYY-MM-DD\_hh-mm-ss} directories
    \begin{itemize}
    \item {\tt system\_info}
    \item ``suite name''.result
    \end{itemize}
  \end{itemize}
\end{itemize}

{\tt system\_info}
\begin{verbatim}
{ host_name: "farm runner 1",
  ocaml_config: {
    key: value,
  }
  ocaml_version: {
    base: "4.02+...",
    commit: "git:1234AEF",
    clean_repository: false
  }
  cpu: { count: 4,
         model: "intel ..." },
  memory: { size: 4096 },
  os: { virtualized: true,
        kind: "Linux",
        version: "3.13..." } }
\end{verbatim}

\begin{itemize}
\item ocaml\_config fields are fields from ocamlopt -config
\item ocaml\_version commit and clean\_repository fields are
  optional.  they are provided if the running directory is in the
  ocaml source tree and it is a git or svn repository. It is the last
  commit identifier. If the repository is not completely committed
  (told by git status), clean\_repository is false
\item On linux cpu model comes from /proc/cpuinfo ``model name'' field
\item memory size is in MB
\item os kind is the result of {\tt uname} on Unices or ``Windows''
\end{itemize}

\subsubsection{Format of the Result Files}\label{result-format}

\begin{verbatim}
{ name: "benchmark name",
  date: "YYYY-MM-DD_hh-mm-ss",
  runs:
    [ { name: "some function",
        description: "...",
        list:
          [ { count: 1000, time: 0.00123, cycles: 1234567,
              minor_words: 123, major_words: 1,
              compactions: 0, ... },
            { count: 10000, time: 0.0123, ... },
            ... ] },
      ... ] }
\end{verbatim}

The list field contains information about each launch. the records
contains a mandatory {\tt count} field indicating the number of times
the test was run. The time fields is the number of seconds running all
the tests took. Each other fields are a optional. Potential ones are:
\begin{itemize}
\item {\tt cycles}: Number of cpu cycles. Recovered using perf like
  tools for macro benchmark and rtdsc instruction on x86 for micro
  benchmarks.
\item {\tt minor\_words}, {\tt major\_words}, ...: Statistics provided
  by {\tt Gc.print\_stat}
\end{itemize}

\subsubsection{Format of the Comparison Results}

A human readable result. Either textual statistical information or a graph.
~\vspace{4cm}~
\subsection{Macro-benchmarking in the Compiler Directory}

\subsubsection{The {\tt operf-ocaml} tool}

\begin{itemize}
\item {\tt operf-ocaml init {\em name} [--repository url] [configure options]}\\
  Prepares the environment for macro benchmarking the
  compiler in the same directory:
  \begin{itemize}
  \item Creates an {\tt operf/macro} sub-directory.
  \item Creates and initialize a local opam repository in the {\tt
    operf/opam-repository} sub-directory:
    \begin{itemize}
    \item Create the compiler descriptions {\tt ``version''-local.descr}
      {\tt ``version''-local.comp} in {\tt
        operf/opam-repository/compilers/}.  the version is provided by
      the VERSION file in the ocaml source tree. The .comp file
      contains no build command.
    \end{itemize}
  \item Run {\tt opam init --root operf/opamroot/ --comp
    ``version''-local local operf/opam-repository/}
  \item Run {\tt opam repository add default ``url''}\\ if
    [--repository ``url''] is specified use it as url otherwise use
    {\tt https://opam.ocaml.org}
  \item Run {\tt opam switch install local --alias ``version''-local}
  \end{itemize}
\item {\tt operf-ocaml configure [configure options]}\\ Run {\tt
  ./configure --prefix operf/opamroot/local/ [configure
    options]}. Should fail if {\tt --prefix} is specified in the
  arguments
\item {\tt operf-ocaml build [all | list of benchmark suites]}\\
  Creates the {\tt operf/macro/timestamp} file containing the
  current time.
  Run {\tt opam install --switch local [list of benchmark
      suites]}\\ If {\tt all} is used, every package tagged with
  ``benchmark'' is installed.
\item {\tt operf-ocaml list}\\ Print the list of available opam
  packages containing the ``benchmark'' tag.
\item {\tt operf-ocaml clean}\\ Reset to the state after init:\\
  \begin{itemize}
  \item {\tt opam switch remove local}
  \item {\tt opam switch install local --alias ``version''-local}
  \end{itemize}
\item {\tt operf-ocaml run [--fast | --slow | --all] [--name ``benchmark name''] [-o ``path'']}\\
  Run benchmarks found in {\tt operf/opamroot/local/share/operf/macro/}.\\
  If [--name] is not present, run every benchmarks.
  otherwise run only those specified.

  If {\tt -o path} is specified, record results in this directory,
  otherwise record it in {\tt
    \$HOME/.operf/results/{\em name}/{\em date}/}\footnote{TODO: XDG}
  (the date is the one recorded in {\tt operf/macro/timestamp}.

\end{itemize}

~\vspace{4cm}~

\subsubsection{Format of the Macro Benchmarks}

A macro benchmark is a program and a description file. The description
file named, {\tt ``name''.bench} has the format
\begin{verbatim}
{ description: "...",
  program: "path",
  arg: { fast: "0-10", slow: ["12", "25"], slower: "any" } }
\end{verbatim}

{\tt .bench} files are found in {\tt \$PREFIX/share/operf/macro/}

\begin{itemize}
\item {\tt program} is the path \footnote{absolute or relative to this
  file ? which one is more practical ?} to the benchmark program.
\item {\tt arg} is optional. It contains range of arguments for which
  the benchmark is fast, slow or very slow. The format is:
  \begin{itemize}
  \item \begin{verbatim}"n"\end{verbatim} a single integer
  \item \begin{verbatim}"n-m"\end{verbatim} an integer range
  \item \begin{verbatim}["n1","n2",...]\end{verbatim} a list of integers
  \item \begin{verbatim}"any"\end{verbatim} the set $\{0, \ldots, max\_int\}$
  \end{itemize}
  It has the same meaning as those specified in {\tt micro\_bench\_type.mli}\ref{micro_bench_type.mli}
\end{itemize}

The program command line is:
{\tt program [-o path] [-arg n]}
\begin{itemize}
\item {\tt -o path} specify where the results are saved. The path is a
  filename. The result is in the format specified in section
  \ref{result-format}.
\item {\tt -arg n} is optional. It can be specified only if the
  .bench file contains an arg field.
\end{itemize}

\subsubsection{Format of the {\tt \$HOME/.operf/macro/} directory}
~\vspace{4cm}~

\subsubsection{Format of the Result Files}

The same format as micro benchmark results, but run fields may differ
depending on what is available and macro benchmark inputs.
~\vspace{4cm}~

\subsection{Macro-benchmarking an OPAM Switch}

\subsubsection{The {\tt operf-opam} tool}

This tool has the same behavior as {\tt operf-ocaml}, except that,
instead of being used in a compiler directory, it will install and run
the macro benchmarks on the current OPAM switch (or one provided on
the command line).

\begin{itemize}
\item {\tt operf-opam [--switch {\em OPAM-SWITCH} ] install {\em
    benchmark} [...]}\\ Install the specified {\tt\em benchmarks} in
  the current switch (or {\tt\em OPAM-SWITCH} if
  provided).\footnote{It is exactly equivalent to opam install. Is it really useful ?}
\item {\tt operf-opam [--switch {\em OPAM-SWITCH} ] run [{\em benchmark} ...] [-o ``path'']}\\
  Run the installed {\tt\em benchmarks} (or all the installed one, if none
  is specified) and save the corresponding results.
\item {\tt operf-opam build-run --repository {\em REPOSITORY} {\em
    VERSION} [{\em benchmark} ...] [-o {\em PATH}]}\\ A single command
  to build, run, save and clean.  It creates a local opam root like
  operf-ocaml build the compiler {\em VERSION} inside, run all
  specified benchmarks, save the result to {\em PATH} or to operf
  result directory and finally remove the local opam root.
\end{itemize}

\subsection{Weather service}
~\vspace{4cm}~

\section{Miscellaneous Ideas}

Provides a more complete idea of the performance of a functional
ocaml source tree.

Basic aim:
\begin{itemize}
\item Micro-benchmarking should be as fast and simple as possible.
\item Global benchmarking and weather service should share their sets
  of benchmarks. This set should be easy to extend.
\item Weather services should require as few maintenance as possible.
\end{itemize}

There are two distinct kinds of benchmarks, \emph{micro-benchmarks} (a specific
function) and \emph{macro-benchmarks} (a program).

\subsection{Description of a global benchmarks}

A benchmark is a specific OPAM package, it is not included in the
library or program package. The install instruction copy the resulting
data to a directory fixed by convention. (probably
\texttt{\%\{prefix\}\%/bench/package\_name})

The actual benchmarking is performed by the \texttt{operf-macro} tool
using the artifacts installed by the OPAM package of the benchmark,
comprising a \texttt{.bench} file, and maybe other binaries and/or
data files.

A package generating benchmark results should be annotated with the
\texttt{benchmark} tag.

\begin{itemize}
\item No or minimal change to opam are required.
\item Allow to add additional dependencies to the benchmark that are
  not required by the original package.
\end{itemize}

Potential problems:
\begin{itemize}
\item Benchmarking functions not available in a module interface is
  not directly possible. It can be circumvented by having a benchmark
  package containing the library sources (more package management
  work), or by exposing a benchmarkable value in the interfaces (can
  be done only by the library developer).
\end{itemize}

\subsubsection{Useful opam extension: multipackage}\label{opam-multipackage}

Another way to specify a benchmark could be to change opam packages to
allow multiple packages to share the same description. This could be
done through the addition of {\tt targets} to the opam file.

For instance, given the following original package:

\begin{verbatim}
build: [
  ["./configure" "--prefix" prefix]
  [make]
  [make "install"]
]
remove: [["ocamlfind" "remove" "mylib"]]
depends: ["ocamlfind" "library1" "library2"]
\end{verbatim}

Adding the target doc and benchmark:

\begin{verbatim}
targets: ["documentation", "benchmark"]
build: [
  ["./configure" "--prefix" prefix]
  [make]
  [make "install"]
]
depends: ["ocamlfind" "library1" "library2"]
remove: [["ocamlfind" "remove" "mylib"]]

build-documentation: [
  ["./configure" "--prefix" prefix]
  [make doc]
  [make "install-doc"]
]
depends-documentation: ["ocamlfind" "library1" "library2" "ocamldoc"]
remove-documentation: [[make "uninstall-doc"]]

build-benchmark: [
  ["./configure" "--prefix" prefix]
  [make]
  [make bench]
  ["cp" "-r" "bench_result" "%{share}%/benchmark/%{package-name}%/%{version}%"]
]
depends-benchmark: ["ocamlfind" "library1" "library2" {>= 1.3} "core_bench"]
remove-benchmark: ["rm" "-rf" "%{share}%/benchmark/%{package-name}%/%{version}%"]
\end{verbatim}

This single opam file would describe, the package ``mylib'' but also
``mylib-documentation'' and ``mylib-benchmark''.

To distinguish with a specified ``mylib-benchmark'' package and allow
to chose which one to install (with opam pin), the version of this
kind of package would be suffixed with ``+target'', and discrete
packages with this suffix would be forbidden. To ensure that the
``-target'' packages are not installed with a different version of the
library or program, implicit conflicts are added (the same conflicts
that prevents two versions of a package to be installed at the same
time).

Note: it could be also be interesting to separate the ``./configure'',
``make'' and ``make install'' part of the build command in three
subcommands.

Advantages
\begin{itemize}
\item Smaller opam repository maintenance burden (less files to edit).
\item Could serve to also handle tests: The problem with testing is
  that it is often easier to run them in the directory where the
  library or program was built, because some tested functions could be
  hidden by interfaces or some useful data are kept in the source
  tree. With this proposition, it is simple to define such testing
  packages without duplicating package descriptions.

  Compared to the current ``build-test'' opam feature it have a saner
  dependency semantics, it does not force to rebuild all the package
  depending on the tested package, and allows to have a completely
  different configuration command than the build (enabling inline
  tests, high verbosity, ...).
\end{itemize}

There is no loss of flexibility: if a completely different tarball is
used for benchmarking, it is still possible to hand-define a
benchmarking package like in the previous proposition.

Potential problems:
\begin{itemize}
\item Increased compilation time when systematically building the
  documentation. There may be some reasonable solutions to build the
  documentation where the library has been built in some common cases.
\item more changes to opam
\end{itemize}

\subsection{Macro-benchmark Runner}

Input:
\begin{itemize}
\item a binary path
\item expected result
\item variables, their encoding and potential values
\end{itemize}

Output:
\begin{itemize}
\item A benchmark file.
\item Additional information to analyze the results provided by
  tools like \texttt{perf record} on linux or \texttt{callgrind}
\end{itemize}

Note: Reporting running time can be done simply in a cross platform
way.

Reporting more precise result like cycles is easy on linux but
other systems would require some search. GC statistics requires
changes to the original program. A possibility would be to adopt as
convention that the environment variable
\texttt{OCAML\_GC\_STATISTICS} contains a file where statistics should
be written (using Gc.print\_stat).

Defining a macrobenchmark package of a program built by an existing
opam package should not requires to provides additional sources (a
url file in the opam package).

\subsection{Microbenchmark runner}

Runs a set of functions, test the results, records the
timings/cycles/gc information and write raw results to the directory
fixed by convention.

Probably the runner part of core\_bench. This should be extracted from
core\_bench (3 files) to minimize dependencies, especially camlp4.

This should be provided as a template package, already containing
build system and base files. The user should only provide a .ml file
containing a list of functions to test and a file containing the list
of build dependencies (the ocamlfind -package part of the compilations
command).

This should allow to provide a microbenchmark as a simple opam package
containing those 2 files in the \texttt{files} subdirectory.

\subsection{Simple microbenchmark set}

A set of microbenchmarks able to run everywhere without any difference
should be provided to reliably test the compiler. It should not depend
on any kind of code generation, or feature specific to a version.

\subsection{Management}

\begin{itemize}
\item List packages containing benchmarks
\item Prepare an opam switch to install a manually built compiler
\item One command compile/run/recover/clean for a given opam compiler
  package and set of benchmarks.
\end{itemize}

\subsection{Aggregation}

This tool should produce the set of graphs given a set of raw
results. It will probably use core\_bench for the statistics part. It
should produce the final html pages to display on the public site.

\subsection{Results management}

Use github: no tool needed.

Benchmark results should be managed in a centralized git repositories,
probably using github to minimize maintenance. Users publishing
results on their exotic systems should send patches/pull requests.

Common architectures results should always be provided by the same
computers to allow easier comparisons, all results should be
automatically produced and committed to the repository.

\section{Needed opam extensions}

\subsection{multipackage}
see section \ref{opam-multipackage}

\subsection{}

\section{Related work}

\href{http://speed.pypy.org/}{pypy speed site}\\
\href{https://github.com/janestreet/core_bench}{Core\_bench}\\
SPEC\\

\end{document}
